{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/workspace/twitter_topic_analysis_dashboard/spark.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "tar: spark-2.4.7-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/gitpod/.pyenv/versions/3.8.13/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-2.4.7-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=157'>158</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=158'>159</a>\u001b[0m     py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(spark_python, \u001b[39m\"\u001b[39;49m\u001b[39mlib\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpy4j-*.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=159'>160</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/twitter_topic_analysis_dashboard/spark.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mSPARK_HOME\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/content/spark-2.4.7-bin-hadoop2.7\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000003vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000003vscode-remote?line=5'>6</a>\u001b[0m findspark\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m      <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000003vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[1;32m     <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000003vscode-remote?line=9'>10</a>\u001b[0m spark_conf \u001b[39m=\u001b[39m SparkConf()\\\n\u001b[1;32m     <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000003vscode-remote?line=10'>11</a>\u001b[0m   \u001b[39m.\u001b[39msetAppName(\u001b[39m\"\u001b[39m\u001b[39mYourTest\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell://youheekil-twittertopica-wj6g2sp5ucy.ws-us46.gitpod.io/workspace/twitter_topic_analysis_dashboard/spark.ipynb#ch0000003vscode-remote?line=11'>12</a>\u001b[0m   \u001b[39m.\u001b[39msetMaster(\u001b[39m\"\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=158'>159</a>\u001b[0m         py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(spark_python, \u001b[39m\"\u001b[39m\u001b[39mlib\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpy4j-*.zip\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=159'>160</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=160'>161</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=161'>162</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to find py4j in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=162'>163</a>\u001b[0m                 spark_python\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=163'>164</a>\u001b[0m             )\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=164'>165</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=165'>166</a>\u001b[0m     sys\u001b[39m.\u001b[39mpath[:\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m sys_path \u001b[39m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=166'>167</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages/findspark.py?line=167'>168</a>\u001b[0m     \u001b[39m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-2.4.7-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark_conf = SparkConf()\\\n",
    "  .setAppName(\"YourTest\")\\\n",
    "  .setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Spark\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('1', 100), ('2', 200), ('3', 300)]).toDF(\"id\", \"value\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
